# Intro

Text summarization (Abstractive Summary)
We utilize state-of-the-art techniques for abstractive text summarization. This project involves developing a summarization model based on the google/mt5-small architecture from Hugging Faceâ€™s Transformers library. The model will be fine-tuned specifically for the summarization task. When an article or document is passed through the network, it generates a concise and coherent summary.

# Technologies

PyTorch, Hugging Face Transformers, mT5, WandB, Tokenization, DataLoader, Fine-Tuning, Text Summarization, NLP


# Dataset
<br>
we use  a small chunk of **Wikihow Dataset** ("Not cleaned dataset for Arabic lang")<br>
and we tried to clean it  .<br>
we used "300" articles for our model <br>
The link to the whole dataset<br>
https://drive.google.com/file/d/1PM7GFCy2gJL1WHqQz1dzqIDIEN6kfRoi/view<br>
we will upload our cleaned chunk of a dataset that contains **11k** articles to the dataset folder <br>
but we used only **300** For the traning <br>
