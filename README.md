# Intro

Text summarization (Abstractive Summary)
We use the latest technology (state-of-the-art technique) for the Text summarization. 
This project involves building a text summarization model using the google/mt5-small model from Hugging Faceâ€™s transformers library.
We will be fine-tuning a transformer model for Summarization Task. a summary of a given article/document is generated when passed through a network.

# Technologies

PyTorch, Hugging Face Transformers, mT5, WandB, Tokenization, DataLoader, Fine-Tuning, Text Summarization, NLP


# Dataset
<br>
we use  a small chunk of **Wikihow Dataset** ("Not cleaned dataset for Arabic lang")<br>
and we tried to clean it  .<br>
we used "300" articles for our model <br>
The link to the whole dataset<br>
https://drive.google.com/file/d/1PM7GFCy2gJL1WHqQz1dzqIDIEN6kfRoi/view<br>
we will upload our cleaned chunk of a dataset that contains **11k** articles to the dataset folder <br>
but we used only **300** because it will take a huge time to train in collab with **300** it takes 1 hour<br>
