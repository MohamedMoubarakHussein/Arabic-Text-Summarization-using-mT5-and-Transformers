{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca8nIFV9PKmH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "!pip install SentencePiece -q\n",
        "!wandb login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc1hTyFZP1yj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import TFMT5Model, T5Tokenizer , MT5ForConditionalGeneration\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g8KNDCQlSw9C"
      },
      "outputs": [],
      "source": [
        "# Create a custom dataset for reading the dataframe and loading it into the dataloader \n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Lyj4ulcYTWth"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone().detach()\n",
        "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%10==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "awqC8lwPTuyA"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNSCMA9DW2HP",
        "outputId": "754bd3db-1969-4f9b-ca2a-c54d853a805d"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Small2_dataset.csv\" ,encoding=\"utf8\")\n",
        "df['Text'] = \"summarize: \" + df['Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw27Xc2pYVtD",
        "outputId": "4baa7e4e-54de-4bd8-f1be-69d2bd59e957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               ctext  \\\n",
            "0  summarize: افرد ذراعك أمامك مع ثني الكوع. يجب ...   \n",
            "1  summarize: قد تتمكن من جعل طعم الأرز العادي يش...   \n",
            "2  summarize: الكلب المصاب بالجفاف سيعمل على الوص...   \n",
            "3  summarize: ينبغي التحضير بحسب إذا كان الجو شدي...   \n",
            "4  summarize: لتنظيف أنفك، سد فتحة أنف واحدة بالم...   \n",
            "\n",
            "                                                text  \n",
            "0  ضع المرطب على شكل خط أعلى كل من الساعدين وظهر ...  \n",
            "1  اعلم ما يجب عليك توقعه. غط القدر بغطاء ودعه يغ...  \n",
            "2  راقب سلوك الكلب. افحص مؤخرة عنق الكلب. افحص لث...  \n",
            "3  تأكد من ارتداء أطفالك لملابس مناسبة. تعرف على ...  \n",
            "4  تنظيف أنفك بشكل صحيح. الحصول على الراحة. الحصو...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('Small2_dataset.csv',encoding='utf-8')\n",
        "df.columns = ['ctext', 'text']\n",
        "df.ctext = 'summarize: ' + df.ctext\n",
        "df['ctext']=df['ctext'][:30]\n",
        "df['text']=df['text'][:30]\n",
        "\n",
        "df['ctext'][3] = df['ctext'][29]\n",
        "df['text'][3] = df['text'][29]\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "VWx5v8eNT3s-",
        "outputId": "ca0ea692-f200-46d1-fbff-d1acd0e7c5f6"
      },
      "outputs": [],
      "source": [
        "\n",
        "wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "config = wandb.config         \n",
        "config.TRAIN_BATCH_SIZE = 1  \n",
        "config.VALID_BATCH_SIZE = 1    \n",
        "config.TRAIN_EPOCHS = 1        \n",
        "config.VAL_EPOCHS = 1 \n",
        "config.LEARNING_RATE = 1e-4    \n",
        "config.SEED = 42               \n",
        "config.MAX_LEN = 512\n",
        "config.SUMMARY_LEN = 150 \n",
        "\n",
        "\n",
        "torch.manual_seed(config.SEED) \n",
        "np.random.seed(config.SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create of Dataset and Dataloader\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset.columns = train_dataset.columns.str.strip()\n",
        "\n",
        "\n",
        "#  create dataset ready to use in  Dataloader\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "# parameters fo dataloaders\n",
        "train_params = {\n",
        "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': config.VALID_BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "# Create  Dataloaders \n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
        "model = model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N9pTmtDglxQF",
        "outputId": "016b2477-7880-4519-90ff-58f72fb3e443"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Defining the optimizer that \n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "\n",
        "\n",
        "print('Fine-Tuning for the model on our dataset')\n",
        "\n",
        "for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(config.VAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv('./models/predictions.csv')\n",
        "    print('Output Files generated for review')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4tdTK9PCiOK"
      },
      "outputs": [],
      "source": [
        "string = input(\"Enter text\")\n",
        "Test = CustomDataset([string , \"\"], tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "\n",
        "Test_loader = DataLoader(val_set, {\n",
        "    'batch_size': config.VALID_BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    } )\n",
        "\n",
        "predictions, actuals = validate(epoch, tokenizer, model, device, Test_loader)\n",
        "print('Generated Text' ,predictions)\n",
        "print('Actual Text',actuals)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
